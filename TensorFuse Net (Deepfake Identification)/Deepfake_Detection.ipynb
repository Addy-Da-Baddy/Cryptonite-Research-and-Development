{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from scipy.fftpack import dct\n",
    "import pickle\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Feature Extraction Functions\n",
    "def extract_dct_features(image_tensor, block_size=8):\n",
    "    # Ensure we have a single image (C,H,W)\n",
    "    if len(image_tensor.shape) == 4:  # If batched, take first image\n",
    "        image_tensor = image_tensor[0]  # Now shape is [C,H,W]\n",
    "    \n",
    "    # Convert to numpy array in HWC format\n",
    "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    image_gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    h, w = image_gray.shape\n",
    "    dct_features = np.zeros((h//block_size, w//block_size, block_size*block_size))\n",
    "    \n",
    "    for i in range(0, h, block_size):\n",
    "        if i + block_size > h:\n",
    "            continue\n",
    "        for j in range(0, w, block_size):\n",
    "            if j + block_size > w:\n",
    "                continue\n",
    "            block = image_gray[i:i+block_size, j:j+block_size]\n",
    "            dct_block = dct(dct(block.T, norm='ortho').T, norm='ortho')\n",
    "            dct_features[i//block_size, j//block_size, :] = dct_block.flatten()\n",
    "    \n",
    "    # Return as tensor with proper dimensions [B, C, H, W]\n",
    "    dct_tensor = torch.from_numpy(dct_features).float().to(device)\n",
    "    dct_tensor = dct_tensor.permute(2, 0, 1).unsqueeze(0)  # [1, C, H, W]\n",
    "    return dct_tensor\n",
    "\n",
    "def extract_fft_features(image_tensor):\n",
    "    # Ensure we have a single image (C,H,W)\n",
    "    if len(image_tensor.shape) == 4:  # If batched, take first image\n",
    "        image_tensor = image_tensor[0]  # Now shape is [C,H,W]\n",
    "        \n",
    "    # Apply FFT to each channel\n",
    "    image_np = image_tensor.cpu().numpy()\n",
    "    fft_features = np.zeros_like(image_np)\n",
    "    \n",
    "    for c in range(image_np.shape[0]):\n",
    "        fft_result = np.fft.fft2(image_np[c])\n",
    "        fft_shifted = np.fft.fftshift(fft_result)\n",
    "        fft_magnitude = np.log(np.abs(fft_shifted) + 1)\n",
    "        fft_features[c] = fft_magnitude\n",
    "    \n",
    "    # Return as tensor with proper dimensions [B, C, H, W]\n",
    "    fft_tensor = torch.from_numpy(fft_features).float().to(device)\n",
    "    fft_tensor = fft_tensor.unsqueeze(0)  # [1, C, H, W]\n",
    "    return fft_tensor\n",
    "\n",
    "def extract_entropy_features(image_tensor, window_size=8):\n",
    "    # Ensure we have a single image (C,H,W)\n",
    "    if len(image_tensor.shape) == 4:  # If batched, take first image\n",
    "        image_tensor = image_tensor[0]  # Now shape is [C,H,W]\n",
    "    \n",
    "    # Convert to numpy array in HWC format\n",
    "    image_np = image_tensor.permute(1, 2, 0).cpu().numpy()\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    image_gray = cv2.cvtColor(image_np, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    h, w = image_gray.shape\n",
    "    entropy_features = np.zeros((h//window_size, w//window_size))\n",
    "    \n",
    "    for i in range(0, h, window_size):\n",
    "        if i + window_size > h:\n",
    "            continue\n",
    "        for j in range(0, w, window_size):\n",
    "            if j + window_size > w:\n",
    "                continue\n",
    "            window = image_gray[i:i+window_size, j:j+window_size]\n",
    "            \n",
    "            # Calculate entropy\n",
    "            hist = cv2.calcHist([window], [0], None, [256], [0, 256])\n",
    "            hist = hist / (window_size * window_size)\n",
    "            non_zero_hist = hist[hist > 0]\n",
    "            entropy = -np.sum(non_zero_hist * np.log2(non_zero_hist)) if len(non_zero_hist) > 0 else 0\n",
    "            \n",
    "            entropy_features[i//window_size, j//window_size] = entropy\n",
    "    \n",
    "    # Return as tensor with proper dimensions [B, C, H, W]\n",
    "    entropy_tensor = torch.from_numpy(entropy_features).float().to(device)\n",
    "    entropy_tensor = entropy_tensor.unsqueeze(0).unsqueeze(0)  # [1, 1, H, W]\n",
    "    return entropy_tensor\n",
    "\n",
    "# Dataset class for real/fake images\n",
    "class RealFakeDataset(Dataset):\n",
    "    def __init__(self, real_dir, fake_dir, transform=None):\n",
    "        self.real_paths = [os.path.join(real_dir, f) for f in os.listdir(real_dir) if f.endswith('.jpg')]\n",
    "        self.fake_paths = [os.path.join(fake_dir, f) for f in os.listdir(fake_dir) if f.endswith('.jpg')]\n",
    "        self.data = [(path, 0) for path in self.real_paths] + [(path, 1) for path in self.fake_paths]\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "# Transform for images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = RealFakeDataset(\n",
    "    r\"/home/jayapal/.cache/kagglehub/datasets/iaddydas/deepfake/versions/1/REAL/REAL\",\n",
    "    r\"/home/jayapal/.cache/kagglehub/datasets/iaddydas/deepfake/versions/1/FAKE/FAKE\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create subset (10%)\n",
    "x = 0.1  # Subset percentage\n",
    "subset_size = int(x * len(full_dataset))\n",
    "indices = torch.randperm(len(full_dataset))[:subset_size]\n",
    "subset = Subset(full_dataset, indices)\n",
    "\n",
    "# Precompute features for subset\n",
    "print(\"Precomputing features for subset...\")\n",
    "precomputed_data = []\n",
    "\n",
    "for idx in tqdm(range(len(subset)), desc=\"Processing images\"):\n",
    "    img, label = subset[idx]\n",
    "    img = img.unsqueeze(0).to(device)  # Add batch dimension [1, C, H, W]\n",
    "    \n",
    "    # Extract features\n",
    "    dct_feat = extract_dct_features(img)\n",
    "    fft_feat = extract_fft_features(img)\n",
    "    entropy_feat = extract_entropy_features(img)\n",
    "    \n",
    "    precomputed_data.append({\n",
    "        'image': img.cpu(),\n",
    "        'dct_features': dct_feat.cpu(),\n",
    "        'fft_features': fft_feat.cpu(),\n",
    "        'entropy_features': entropy_feat.cpu(),\n",
    "        'label': label\n",
    "    })\n",
    "\n",
    "# Save precomputed dataset\n",
    "print(\"Saving precomputed dataset...\")\n",
    "with open('CIFAKE_Precomputed_Dataset.pkl', 'wb') as f:\n",
    "    pickle.dump(precomputed_data, f)\n",
    "\n",
    "# Visualize 4 random samples\n",
    "print(\"\\nVisualizing 4 random samples...\")\n",
    "plt.figure(figsize=(20, 15))\n",
    "random_indices = random.sample(range(len(precomputed_data)), 4)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    sample = precomputed_data[idx]\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(4, 4, i*4 + 1)\n",
    "    img = sample['image'].squeeze(0).permute(1, 2, 0)\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
    "    plt.imshow(img.clamp(0, 1))\n",
    "    plt.title(f\"Original Image\\nLabel: {'Fake' if sample['label'] else 'Real'}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # DCT features\n",
    "    plt.subplot(4, 4, i*4 + 2)\n",
    "    dct_vis = sample['dct_features'].squeeze(0).mean(dim=0)\n",
    "    plt.imshow(dct_vis, cmap='viridis')\n",
    "    plt.title('DCT Features')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # FFT features\n",
    "    plt.subplot(4, 4, i*4 + 3)\n",
    "    fft_vis = sample['fft_features'].squeeze(0).mean(dim=0)\n",
    "    plt.imshow(fft_vis, cmap='viridis')\n",
    "    plt.title('FFT Features')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Entropy features\n",
    "    plt.subplot(4, 4, i*4 + 4)\n",
    "    entropy_vis = sample['entropy_features'].squeeze(0).squeeze(0)\n",
    "    plt.imshow(entropy_vis, cmap='viridis')\n",
    "    plt.title('Entropy Features')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Precomputation and visualization complete!\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load precomputed dataset\n",
    "print(\"Loading precomputed dataset...\")\n",
    "with open('CIFAKE_Precomputed_Dataset.pkl', 'rb') as f:\n",
    "    precomputed_data = pickle.load(f)\n",
    "\n",
    "# Create dataset class for precomputed features\n",
    "class PrecomputedFeatureDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        return (\n",
    "            sample['image'],\n",
    "            sample['dct_features'],\n",
    "            sample['fft_features'],\n",
    "            sample['entropy_features'],\n",
    "            sample['label']\n",
    "        )\n",
    "\n",
    "# Create dataset and split\n",
    "dataset = PrecomputedFeatureDataset(precomputed_data)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# ResNet-style basic block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# NCFIM Model\n",
    "class NCFIMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NCFIMModel, self).__init__()\n",
    "        \n",
    "        # Raw image CNN\n",
    "        self.raw_image_cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Feature processors\n",
    "        self.dct_processor = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fft_processor = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.entropy_processor = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # ResNet backbone\n",
    "        self.stage1 = nn.Sequential(\n",
    "            ResidualBlock(64, 64),\n",
    "            ResidualBlock(64, 64)\n",
    "        )\n",
    "        \n",
    "        self.stage2 = nn.Sequential(\n",
    "            ResidualBlock(64, 128, stride=2),\n",
    "            ResidualBlock(128, 128)\n",
    "        )\n",
    "        \n",
    "        self.stage3 = nn.Sequential(\n",
    "            ResidualBlock(128, 256, stride=2),\n",
    "            ResidualBlock(256, 256),\n",
    "            ResidualBlock(256, 256)\n",
    "        )\n",
    "        \n",
    "        self.stage4 = nn.Sequential(\n",
    "            ResidualBlock(256, 512, stride=2),\n",
    "            ResidualBlock(512, 512),\n",
    "            ResidualBlock(512, 512)\n",
    "        )\n",
    "        \n",
    "        # Final classification\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        \n",
    "    def compute_ncfim(self, feature_list):\n",
    "        batch_size = feature_list[0].size(0)\n",
    "        ncfim = torch.zeros(batch_size, 4, 4, device=feature_list[0].device)\n",
    "        \n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if i != j:  # Skip diagonal\n",
    "                    F_i = feature_list[i]\n",
    "                    F_j = feature_list[j]\n",
    "                    squared_diff = (F_i - F_j)**2\n",
    "                    mean_squared_diff = torch.mean(squared_diff, dim=(1, 2, 3))\n",
    "                    ncfim[:, i, j] = torch.tanh(mean_squared_diff)\n",
    "        \n",
    "        return ncfim\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x is a tuple of (image, dct_features, fft_features, entropy_features)\n",
    "        image, dct_features, fft_features, entropy_features = x\n",
    "        \n",
    "        # Process each feature stream\n",
    "        processed_dct = self.dct_processor(dct_features)\n",
    "        processed_fft = self.fft_processor(fft_features)\n",
    "        processed_entropy = self.entropy_processor(entropy_features)\n",
    "        \n",
    "        # CNN features\n",
    "        cnn_features = self.raw_image_cnn(image)\n",
    "        \n",
    "        # Ensure all features have same shape\n",
    "        feature_list = [processed_dct, processed_fft, processed_entropy, cnn_features]\n",
    "        \n",
    "        # Compute NCFIM\n",
    "        ncfim = self.compute_ncfim(feature_list)\n",
    "        \n",
    "        # Calculate fusion weights\n",
    "        weights = torch.sum(ncfim, dim=2)\n",
    "        \n",
    "        # Fuse features using NCFIM weights\n",
    "        F_final = torch.zeros_like(processed_dct)\n",
    "        for i in range(4):\n",
    "            weight = weights[:, i].view(-1, 1, 1, 1)\n",
    "            F_final += weight * feature_list[i]\n",
    "        \n",
    "        # Pass through ResNet backbone\n",
    "        x = self.stage1(F_final)\n",
    "        x = self.stage2(x)\n",
    "        x = self.stage3(x)\n",
    "        x = self.stage4(x)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, ncfim, feature_list\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = NCFIMModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    train_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]', leave=False)\n",
    "    for images, dct_features, fft_features, entropy_features, labels in train_bar:\n",
    "        # Move data to GPU\n",
    "        images = images.to(device)\n",
    "        dct_features = dct_features.to(device)\n",
    "        fft_features = fft_features.to(device)\n",
    "        entropy_features = entropy_features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs, ncfim, feature_list = model((images, dct_features, fft_features, entropy_features))\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        train_bar.set_postfix({\n",
    "            'loss': running_loss / (train_bar.n + 1),\n",
    "            'acc': 100. * correct / total\n",
    "        })\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = 100. * correct / total\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_bar = tqdm(test_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Test]', leave=False)\n",
    "        for images, dct_features, fft_features, entropy_features, labels in test_bar:\n",
    "            # Move data to GPU\n",
    "            images = images.to(device)\n",
    "            dct_features = dct_features.to(device)\n",
    "            fft_features = fft_features.to(device)\n",
    "            entropy_features = entropy_features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, ncfim, feature_list = model((images, dct_features, fft_features, entropy_features))\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            test_bar.set_postfix({\n",
    "                'loss': test_loss / (test_bar.n + 1),\n",
    "                'acc': 100. * correct / total\n",
    "            })\n",
    "    \n",
    "    test_loss = test_loss / len(test_loader)\n",
    "    test_accuracy = 100. * correct / total\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "    print(f'Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.2f}%')\n",
    "    print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_accuracy:.2f}%')\n",
    "    \n",
    "    # Visualize NCFIM matrix and feature fusion\n",
    "    if epoch % 5 == 0:  # Every 5 epochs\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # NCFIM matrix\n",
    "        plt.subplot(1, 3, 1)\n",
    "        ncfim_vis = ncfim[0].cpu().numpy()\n",
    "        plt.imshow(ncfim_vis, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.colorbar()\n",
    "        plt.title('NCFIM Matrix')\n",
    "        plt.xticks([0,1,2,3], ['DCT','FFT','Entropy','CNN'])\n",
    "        plt.yticks([0,1,2,3], ['DCT','FFT','Entropy','CNN'])\n",
    "        \n",
    "        # Feature weights\n",
    "        plt.subplot(1, 3, 2)\n",
    "        weights = torch.sum(ncfim, dim=2)[0].cpu().numpy()\n",
    "        plt.bar(['DCT','FFT','Entropy','CNN'], weights)\n",
    "        plt.title('Feature Weights')\n",
    "        plt.ylabel('NCFIM Weight')\n",
    "        \n",
    "        # Confusion matrix\n",
    "        plt.subplot(1, 3, 3)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.title('Training and Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, matthews_corrcoef, cohen_kappa_score, log_loss, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the saved model\n",
    "model_path = 'real_fake_fusion_model.pth'\n",
    "model = MultiDomainFeatureFusionModel().to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "# Function to get all predictions and true labels\n",
    "def get_all_predictions(model, loader):\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    all_inputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(loader, desc='Evaluating'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Get probabilities and predictions\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_inputs.append(inputs.cpu())\n",
    "            all_probs.append(probs.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    \n",
    "    return torch.cat(all_inputs), torch.cat(all_preds), torch.cat(all_probs), torch.cat(all_labels)\n",
    "\n",
    "# Get predictions for both datasets\n",
    "print(\"Evaluating train dataset:\")\n",
    "train_inputs, train_preds, train_probs, train_labels = get_all_predictions(model, train_loader)\n",
    "print(\"Evaluating test dataset:\")\n",
    "test_inputs, test_preds, test_probs, test_labels = get_all_predictions(model, test_loader)\n",
    "\n",
    "# Convert to numpy for both datasets\n",
    "train_y_true = train_labels.numpy()\n",
    "train_y_pred = train_preds.numpy()\n",
    "train_y_probs = train_probs.numpy()[:, 1]  # Probability of class 1 (fake)\n",
    "\n",
    "test_y_true = test_labels.numpy()\n",
    "test_y_pred = test_preds.numpy()\n",
    "test_y_probs = test_probs.numpy()[:, 1]  # Probability of class 1 (fake)\n",
    "\n",
    "# 1. Classification Report for both datasets\n",
    "print(\"\\nClassification Report (Training Set):\")\n",
    "print(classification_report(train_y_true, train_y_pred, target_names=['Real', 'Fake']))\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_y_true, test_y_pred, target_names=['Real', 'Fake']))\n",
    "\n",
    "# 2. Confusion Matrix using matplotlib instead of seaborn\n",
    "def plot_confusion_matrix(y_true, y_pred, dataset_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix - {dataset_name}')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    classes = ['Real', 'Fake']\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.ylabel('True label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(train_y_true, train_y_pred, \"Training Set\")\n",
    "plot_confusion_matrix(test_y_true, test_y_pred, \"Test Set\")\n",
    "\n",
    "# 3. ROC Curve and AUC for both datasets\n",
    "def plot_roc_curve(y_true, y_probs, dataset_name):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic - {dataset_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_roc_curve(train_y_true, train_y_probs, \"Training Set\")\n",
    "plot_roc_curve(test_y_true, test_y_probs, \"Test Set\")\n",
    "\n",
    "# 4. Precision-Recall Curve for both datasets\n",
    "def plot_precision_recall(y_true, y_probs, dataset_name):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_probs)\n",
    "    avg_precision = average_precision_score(y_true, y_probs)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2,\n",
    "             label=f'Precision-Recall (AP = {avg_precision:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve - {dataset_name}')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "plot_precision_recall(train_y_true, train_y_probs, \"Training Set\")\n",
    "plot_precision_recall(test_y_true, test_y_probs, \"Test Set\")\n",
    "\n",
    "# 5. Advanced Metrics Table for both datasets\n",
    "def calculate_advanced_metrics(y_true, y_pred, y_probs):\n",
    "    # Calculate ROC curve values for AUC\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_probs)\n",
    "    roc_auc_value = auc(fpr, tpr)\n",
    "    \n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Balanced Accuracy': balanced_accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred),\n",
    "        'Recall': recall_score(y_true, y_pred),\n",
    "        'F1 Score': f1_score(y_true, y_pred),  # Explicitly using f1_score from sklearn.metrics\n",
    "        'MCC': matthews_corrcoef(y_true, y_pred),\n",
    "        \"Cohen's Kappa\": cohen_kappa_score(y_true, y_pred),\n",
    "        'Log Loss': log_loss(y_true, y_probs),\n",
    "        'ROC AUC': roc_auc_value,\n",
    "        'PR AUC': average_precision_score(y_true, y_probs)\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame.from_dict(metrics, orient='index', columns=['Value'])\n",
    "\n",
    "train_advanced_metrics = calculate_advanced_metrics(train_y_true, train_y_pred, train_y_probs)\n",
    "print(\"\\nAdvanced Metrics (Training Set):\")\n",
    "print(train_advanced_metrics)\n",
    "\n",
    "test_advanced_metrics = calculate_advanced_metrics(test_y_true, test_y_pred, test_y_probs)\n",
    "print(\"\\nAdvanced Metrics (Test Set):\")\n",
    "print(test_advanced_metrics)\n",
    "\n",
    "# 6. Class-wise Metrics for both datasets\n",
    "def class_wise_metrics(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Handle potential division by zero\n",
    "    def safe_division(x, y):\n",
    "        return x / y if y != 0 else 0\n",
    "    \n",
    "    metrics = {\n",
    "        'Class': ['Real', 'Fake'],\n",
    "        'TP': [tn, tp],  # For Real, TN is like TP\n",
    "        'FP': [fp, fn],  # For Real, FP is like FN\n",
    "        'TN': [tp, tn],  # For Real, TP is like TN\n",
    "        'FN': [fn, fp],  # For Real, FN is like FP\n",
    "        'Sensitivity': [safe_division(tn, (tn+fp)), safe_division(tp, (tp+fn))],\n",
    "        'Specificity': [safe_division(tp, (tp+fn)), safe_division(tn, (tn+fp))],\n",
    "        'PPV': [safe_division(tn, (tn+fn)), safe_division(tp, (tp+fp))],\n",
    "        'NPV': [safe_division(tp, (tp+fn)), safe_division(tn, (tn+fp))]\n",
    "    }\n",
    "    \n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "train_class_metrics = class_wise_metrics(train_y_true, train_y_pred)\n",
    "print(\"\\nClass-wise Metrics (Training Set):\")\n",
    "print(train_class_metrics)\n",
    "\n",
    "test_class_metrics = class_wise_metrics(test_y_true, test_y_pred)\n",
    "print(\"\\nClass-wise Metrics (Test Set):\")\n",
    "print(test_class_metrics)\n",
    "\n",
    "# 7. Error Analysis for both datasets\n",
    "def analyze_errors(model, inputs, preds, labels, dataset_name, num_samples=5):\n",
    "    # Find misclassified samples\n",
    "    incorrect_mask = ~preds.eq(labels)\n",
    "    incorrect_indices = torch.where(incorrect_mask)[0]\n",
    "    \n",
    "    if len(incorrect_indices) == 0:\n",
    "        print(f\"No misclassifications found in the {dataset_name}\")\n",
    "        return\n",
    "    \n",
    "    # Limit to the requested number of samples\n",
    "    sample_count = min(num_samples, len(incorrect_indices))\n",
    "    selected_indices = incorrect_indices[:sample_count]\n",
    "    \n",
    "    # Visualize errors\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, idx in enumerate(selected_indices):\n",
    "        plt.subplot(1, sample_count, i+1)\n",
    "        plt.imshow(inputs[idx].permute(1, 2, 0))\n",
    "        plt.title(f\"True: {'Fake' if labels[idx] else 'Real'}\\n\"\n",
    "                 f\"Pred: {'Fake' if preds[idx] else 'Real'}\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Error Analysis - {dataset_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nError Analysis (Training Set):\")\n",
    "analyze_errors(model, train_inputs, train_preds, train_labels, \"Training Set\")\n",
    "\n",
    "print(\"\\nError Analysis (Test Set):\")\n",
    "analyze_errors(model, test_inputs, test_preds, test_labels, \"Test Set\")\n",
    "\n",
    "# 8. Threshold Analysis for both datasets\n",
    "def threshold_analysis(y_true, y_probs, dataset_name):\n",
    "    thresholds = np.linspace(0, 1, 101)\n",
    "    metrics = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        y_pred_thresh = (y_probs >= thresh).astype(int)\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred_thresh)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        \n",
    "        # Handle division by zero\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        \n",
    "        # Calculate F1 score explicitly to avoid potential issues\n",
    "        f1 = 2 * (precision * tpr) / (precision + tpr) if (precision + tpr) > 0 else 0\n",
    "        \n",
    "        metrics.append({\n",
    "            'Threshold': thresh,\n",
    "            'TPR': tpr,\n",
    "            'FPR': fpr,\n",
    "            'Precision': precision,\n",
    "            'F1': f1\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(metrics)\n",
    "    \n",
    "    # Plot metrics vs threshold\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['Threshold'], df['TPR'], label='True Positive Rate (Recall)')\n",
    "    plt.plot(df['Threshold'], df['FPR'], label='False Positive Rate')\n",
    "    plt.plot(df['Threshold'], df['Precision'], label='Precision')\n",
    "    plt.plot(df['Threshold'], df['F1'], label='F1 Score')\n",
    "    \n",
    "    # Find optimal threshold (max F1)\n",
    "    if df['F1'].max() > 0:\n",
    "        optimal_idx = df['F1'].idxmax()\n",
    "        optimal_thresh = df.loc[optimal_idx, 'Threshold']\n",
    "        plt.axvline(x=optimal_thresh, color='r', linestyle='--', \n",
    "                    label=f'Optimal Threshold: {optimal_thresh:.2f}')\n",
    "    \n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.title(f'Threshold Analysis - {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    optimal_threshold = df.loc[df['F1'].idxmax(), 'Threshold'] if df['F1'].max() > 0 else 0.5\n",
    "    return optimal_threshold\n",
    "\n",
    "print(\"\\nThreshold Analysis (Training Set):\")\n",
    "train_optimal_threshold = threshold_analysis(train_y_true, train_y_probs, \"Training Set\")\n",
    "print(f\"Optimal Decision Threshold (Training Set): {train_optimal_threshold:.4f}\")\n",
    "\n",
    "print(\"\\nThreshold Analysis (Test Set):\")\n",
    "test_optimal_threshold = threshold_analysis(test_y_true, test_y_probs, \"Test Set\")\n",
    "print(f\"Optimal Decision Threshold (Test Set): {test_optimal_threshold:.4f}\")\n",
    "\n",
    "# 9. Prediction Distribution for both datasets\n",
    "def plot_prediction_distribution(y_true, y_probs, optimal_threshold, dataset_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Create bins for histograms\n",
    "    bins = np.linspace(0, 1, 21)\n",
    "    \n",
    "    # Plot histograms for each class\n",
    "    real_probs = y_probs[y_true == 0]\n",
    "    fake_probs = y_probs[y_true == 1]\n",
    "    \n",
    "    if len(real_probs) > 0:\n",
    "        plt.hist(real_probs, bins=bins, alpha=0.5, color='blue', \n",
    "                 label='Real', density=True)\n",
    "    \n",
    "    if len(fake_probs) > 0:\n",
    "        plt.hist(fake_probs, bins=bins, alpha=0.5, color='red', \n",
    "                 label='Fake', density=True)\n",
    "    \n",
    "    plt.axvline(x=0.5, color='black', linestyle='--', label='Default Threshold (0.5)')\n",
    "    plt.axvline(x=optimal_threshold, color='green', linestyle='--', \n",
    "                label=f'Optimal Threshold ({optimal_threshold:.2f})')\n",
    "    \n",
    "    plt.xlabel('Predicted Probability of Being Fake')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title(f'Prediction Distribution by True Class - {dataset_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_prediction_distribution(train_y_true, train_y_probs, train_optimal_threshold, \"Training Set\")\n",
    "plot_prediction_distribution(test_y_true, test_y_probs, test_optimal_threshold, \"Test Set\")\n",
    "\n",
    "# 10. Compare Training vs Testing Performance\n",
    "def compare_datasets_performance():\n",
    "    # Create comparison DataFrame\n",
    "    comparison = pd.DataFrame({\n",
    "        'Metric': train_advanced_metrics.index,\n",
    "        'Training': train_advanced_metrics['Value'],\n",
    "        'Testing': test_advanced_metrics['Value'],\n",
    "        'Difference': test_advanced_metrics['Value'] - train_advanced_metrics['Value']\n",
    "    })\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']\n",
    "    comparison_subset = comparison[comparison['Metric'].isin(metrics_to_plot)]\n",
    "    \n",
    "    x = np.arange(len(metrics_to_plot))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, comparison_subset['Training'], width, label='Training')\n",
    "    plt.bar(x + width/2, comparison_subset['Testing'], width, label='Testing')\n",
    "    \n",
    "    plt.xlabel('Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Training vs Testing Performance')\n",
    "    plt.xticks(x, metrics_to_plot)\n",
    "    plt.legend()\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(comparison_subset['Training']):\n",
    "        plt.text(i - width/2, v + 0.01, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    for i, v in enumerate(comparison_subset['Testing']):\n",
    "        plt.text(i + width/2, v + 0.01, f'{v:.2f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "print(\"\\nTraining vs Testing Performance Comparison:\")\n",
    "performance_comparison = compare_datasets_performance()\n",
    "print(performance_comparison)\n",
    "\n",
    "# 11. Overall Model Assessment Summary\n",
    "def print_model_assessment_summary():\n",
    "    print(\"\\n==== MODEL ASSESSMENT SUMMARY ====\")\n",
    "    \n",
    "    # Calculate overfitting metrics\n",
    "    accuracy_diff = train_advanced_metrics.loc['Accuracy', 'Value'] - test_advanced_metrics.loc['Accuracy', 'Value']\n",
    "    f1_diff = train_advanced_metrics.loc['F1 Score', 'Value'] - test_advanced_metrics.loc['F1 Score', 'Value']\n",
    "    \n",
    "    print(f\"Model: MultiDomainFeatureFusionModel\")\n",
    "    print(f\"Training Accuracy: {train_advanced_metrics.loc['Accuracy', 'Value']:.4f}\")\n",
    "    print(f\"Testing Accuracy: {test_advanced_metrics.loc['Accuracy', 'Value']:.4f}\")\n",
    "    print(f\"Accuracy Gap (Train-Test): {accuracy_diff:.4f}\")\n",
    "    \n",
    "    print(f\"\\nTraining F1 Score: {train_advanced_metrics.loc['F1 Score', 'Value']:.4f}\")\n",
    "    print(f\"Testing F1 Score: {test_advanced_metrics.loc['F1 Score', 'Value']:.4f}\")\n",
    "    print(f\"F1 Score Gap (Train-Test): {f1_diff:.4f}\")\n",
    "    \n",
    "    # Assess overfitting\n",
    "    if accuracy_diff > 0.05:\n",
    "        print(\"\\nPotential overfitting detected: The model performs significantly better on training data than test data.\")\n",
    "        print(\"Consider regularization techniques or collecting more diverse training data.\")\n",
    "    else:\n",
    "        print(\"\\nNo significant overfitting detected: Model performs similarly on training and test data.\")\n",
    "    \n",
    "    # Assess model quality based on test metrics\n",
    "    test_f1 = test_advanced_metrics.loc['F1 Score', 'Value']\n",
    "    if test_f1 > 0.9:\n",
    "        model_quality = \"Excellent\"\n",
    "    elif test_f1 > 0.8:\n",
    "        model_quality = \"Good\"\n",
    "    elif test_f1 > 0.7:\n",
    "        model_quality = \"Fair\"\n",
    "    else:\n",
    "        model_quality = \"Needs improvement\"\n",
    "    \n",
    "    print(f\"\\nOverall Model Quality: {model_quality}\")\n",
    "    print(f\"Optimal Threshold (based on F1): {test_optimal_threshold:.4f}\")\n",
    "    \n",
    "    # Class-specific assessment\n",
    "    real_sensitivity = test_class_metrics.loc[0, 'Sensitivity']\n",
    "    fake_sensitivity = test_class_metrics.loc[1, 'Sensitivity']\n",
    "    \n",
    "    print(f\"\\nReal Image Detection Sensitivity: {real_sensitivity:.4f}\")\n",
    "    print(f\"Fake Image Detection Sensitivity: {fake_sensitivity:.4f}\")\n",
    "    \n",
    "    if abs(real_sensitivity - fake_sensitivity) > 0.1:\n",
    "        imbalance_class = \"Real\" if real_sensitivity > fake_sensitivity else \"Fake\"\n",
    "        print(f\"Class imbalance detected: Model performs better on '{imbalance_class}' class.\")\n",
    "        print(\"Consider balanced training techniques or adjusting the decision threshold.\")\n",
    "\n",
    "print_model_assessment_summary()\n",
    "\n",
    "\n",
    "\n",
    "scp -P 6868 \"C:\\Deepfake\\Deepfake_Detection.ipynb\" harshith@45.112.150.141:/home/harshith/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
