{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dasad\\AppData\\Local\\Temp\\ipykernel_24604\\3746945435.py:21: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(\"HAM10000Skin_Cancer.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 7261, Test size: 1816\n",
      "Classnames: ['Benign lesions of the keratosis', 'Basal cell carcinoma', 'Dermatofibroma', 'Melanoma', 'Melanocytic nevi', 'Vascular lesions', 'Actinic keratoses']\n",
      "Label dictionary: {'bkl': 'Benign lesions of the keratosis', 'bcc': 'Basal cell carcinoma', 'df': 'Dermatofibroma', 'mel': 'Melanoma', 'nv': 'Melanocytic nevi', 'vasc': 'Vascular lesions', 'akiec': 'Actinic keratoses'}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the dataset\n",
    "dataset = torch.load(\"HAM10000Skin_Cancer.pt\")\n",
    "\n",
    "# Extract images and metadata\n",
    "images = dataset[\"images\"]\n",
    "metadata = dataset[\"metadata\"]\n",
    "\n",
    "# Extract labels (assume dx is stored in the first column of metadata)\n",
    "labels = metadata[:, 0].long()\n",
    "\n",
    "# Create a DataFrame for clinical data (excluding labels)\n",
    "clinical_data = pd.DataFrame(metadata[:, 1:].numpy(), columns=[\n",
    "    f\"clinical_{i}\" for i in range(1, metadata.size(1))\n",
    "])\n",
    "\n",
    "# Store labels and class names\n",
    "classnames = [\n",
    "    \"Benign lesions of the keratosis\",\n",
    "    \"Basal cell carcinoma\",\n",
    "    \"Dermatofibroma\",\n",
    "    \"Melanoma\",\n",
    "    \"Melanocytic nevi\",\n",
    "    \"Vascular lesions\",\n",
    "    \"Actinic keratoses\"\n",
    "]\n",
    "label_dict = {abbr: name for abbr, name in zip([\"bkl\", \"bcc\", \"df\", \"mel\", \"nv\", \"vasc\", \"akiec\"], classnames)}\n",
    "\n",
    "# Split data into train and test\n",
    "train_ratio = 0.8\n",
    "train_size = int(train_ratio * len(images))\n",
    "test_size = len(images) - train_size\n",
    "\n",
    "indices = torch.randperm(len(images)).tolist()  # Shuffle indices\n",
    "train_indices, test_indices = indices[:train_size], indices[train_size:]\n",
    "\n",
    "# Split data\n",
    "X_train_images = images[train_indices]\n",
    "X_test_images = images[test_indices]\n",
    "X_train_clin = clinical_data.iloc[train_indices]\n",
    "X_test_clin = clinical_data.iloc[test_indices]\n",
    "y_train = labels[train_indices]\n",
    "y_test = labels[test_indices]\n",
    "\n",
    "# Create a custom dataset class\n",
    "class SkinCancerDataset(Dataset):\n",
    "    def __init__(self, images, clinical_data, labels):\n",
    "        self.images = images\n",
    "        self.clinical_data = clinical_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"image\": self.images[idx],\n",
    "            \"clinical_data\": torch.tensor(self.clinical_data.iloc[idx].values, dtype=torch.float32),\n",
    "            \"label\": self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SkinCancerDataset(X_train_images, X_train_clin, y_train)\n",
    "test_dataset = SkinCancerDataset(X_test_images, X_test_clin, y_test)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Print summary\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "print(f\"Classnames: {classnames}\")\n",
    "print(f\"Label dictionary: {label_dict}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedResnet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, initial_res_weights=0.001):\n",
    "        super(WeightedResnet, self).__init__()\n",
    "\n",
    "        # Initialize res_weights as a learnable parameter\n",
    "        self.res_weights = nn.Parameter(torch.tensor(initial_res_weights, dtype=torch.float32))\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True)\n",
    "        )\n",
    "        self.res1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "        self.res2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128)\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "        )\n",
    "        self.res3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256)\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.res4 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.1, inplace=True),\n",
    "            nn.Linear(256, 512)\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.res1(x) + self.res_weights * x\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.res2(x) + self.res_weights * x\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.res3(x) + self.res_weights * x\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.res4(x) + self.res_weights * x\n",
    "\n",
    "        x = self.classifier(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicalNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ClinicalNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GatedFusion(nn.Module):\n",
    "    def __init__(self, image_model, clinical_model, num_classes):\n",
    "        super(GatedFusion, self).__init__()\n",
    "        self.image_model = image_model\n",
    "        self.clinical_model = clinical_model\n",
    "        self.concat_weights = nn.Parameter(torch.ones(2))  \n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512,num_classes)\n",
    "\n",
    "    def forward(self, image, clinical):\n",
    "        image_features = self.image_model(image)  \n",
    "        clinical_features = self.clinical_model(clinical)  \n",
    "\n",
    "        weighted_image_features = self.concat_weights[0] * image_features\n",
    "        weighted_clinical_features = self.concat_weights[1] * clinical_features\n",
    "        combined_features = torch.cat([weighted_image_features, weighted_clinical_features], dim=1)\n",
    "\n",
    "        x = torch.relu(self.fc1(combined_features))\n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:  96%|██████████████████████████████████████████████▎ | 219/227 [00:15<00:00, 14.61it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set the device for training (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize models\n",
    "num_classes = len(set(labels))\n",
    "resnet_model = WeightedResnet(in_channels=3, num_classes=num_classes).to(device)\n",
    "clinical_nn = ClinicalNN(input_dim=X_train_clin.shape[1], num_classes=num_classes).to(device)\n",
    "combined_model = GatedFusion(image_model=resnet_model, clinical_model=clinical_nn, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.00001)\n",
    "\n",
    "# DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "# Training loop with tqdm and GPU support\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    combined_model.train()  # Set the model to training mode\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Train the model\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\", ncols=100):\n",
    "        images = batch[\"image\"].to(device)\n",
    "        clinical_data = batch[\"clinical_data\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = combined_model(images, clinical_data)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate loss and accuracy\n",
    "        running_train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "    \n",
    "    # Compute average training loss and accuracy for the epoch\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_accuracy = correct_train / total_train * 100\n",
    "\n",
    "    # Periodically check train and test loss every 10 epochs\n",
    "    if epoch % 1 == 0:\n",
    "        # Evaluate on test set\n",
    "        combined_model.eval()  # Set the model to evaluation mode\n",
    "        running_test_loss = 0.0\n",
    "        correct_test = 0\n",
    "        total_test = 0\n",
    "        \n",
    "        with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "            for batch in test_loader:\n",
    "                images = batch[\"image\"].to(device)\n",
    "                clinical_data = batch[\"clinical_data\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = combined_model(images, clinical_data)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_test_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct_test += (predicted == labels).sum().item()\n",
    "                total_test += labels.size(0)\n",
    "        \n",
    "        # Compute average test loss and accuracy\n",
    "        avg_test_loss = running_test_loss / len(test_loader)\n",
    "        test_accuracy = correct_test / total_test * 100\n",
    "\n",
    "        # Print losses and accuracies every 10 epochs\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "        print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")\n",
    "    \n",
    "    # Optionally, save the model at checkpoints\n",
    "    # if epoch % 10 == 0:\n",
    "    #     torch.save(combined_model.state_dict(), f\"checkpoint_epoch_{epoch}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
