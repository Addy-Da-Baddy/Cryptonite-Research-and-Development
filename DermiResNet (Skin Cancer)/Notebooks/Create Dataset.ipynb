{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Paths\n",
    "metadata_path = \"ham_metadata.csv\"\n",
    "images_path = \"HAMImages/\"\n",
    "output_file = \"HAM10000Skin_Cancer.pt\"\n",
    "\n",
    "# Load the metadata\n",
    "metadata = pd.read_csv(metadata_path)# Dictionary for mapping target labels\n",
    "dx_mapping = {\n",
    "    \"bkl\": 0,\n",
    "    \"bcc\": 1,\n",
    "    \"df\": 2,\n",
    "    \"mel\": 3,\n",
    "    \"nv\": 4,\n",
    "    \"vasc\": 5,\n",
    "    \"akiec\": 6,\n",
    "}\n",
    "\n",
    "# Map 'dx' column to numerical labels\n",
    "metadata[\"dx\"] = metadata[\"dx\"].map(dx_mapping)\n",
    "\n",
    "# One-hot encode dx_type, sex, and localization columns\n",
    "metadata = pd.get_dummies(metadata, columns=[\"dx_type\", \"sex\", \"localization\"], prefix=[\"dx_type\", \"sex\", \"local\"])\n",
    "metadata = metadata.replace({True: 1, False: 0})\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression \n",
    "imputer = IterativeImputer(estimator=LinearRegression(), \n",
    "                          max_iter=10, \n",
    "                          random_state=0) \n",
    "\n",
    "\n",
    "metadata['age'] = imputer.fit_transform(metadata[['age']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lesion_id', 'image_id', 'dx', 'dx_type', 'age', 'sex', 'localization'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = pd.read_csv(metadata_path)# Dictionary for mapping target labels\n",
    "metadata.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10015/10015 [07:36<00:00, 21.95it/s]\n",
      "C:\\Users\\dasad\\AppData\\Local\\Temp\\ipykernel_14180\\961584571.py:33: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  metadata_tensor = torch.tensor(metadata_list, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to HAM10000Skin_Cancer.pt\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),         # Convert to tensor\n",
    "])\n",
    "\n",
    "# Initialize lists for storing processed data\n",
    "images = []\n",
    "metadata_list = []\n",
    "nfc = 0\n",
    "fc = 0\n",
    "# Iterate through the metadata\n",
    "for _, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    image_path = os.path.join(images_path, row[\"image_id\"] + \".jpg\")\n",
    "    \n",
    "    # Check if the image file exists\n",
    "    if os.path.exists(image_path):\n",
    "        # Load and preprocess the image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "\n",
    "            # Append metadata (excluding 'image_id' and 'lesion_id')\n",
    "            metadata_list.append(row.drop([\"image_id\", \"lesion_id\"]).values)\n",
    "            fc += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "    else:\n",
    "        nfc += 1\n",
    "        \n",
    "\n",
    "# Convert metadata list to tensor\n",
    "metadata_tensor = torch.tensor(metadata_list, dtype=torch.float32)\n",
    "\n",
    "# Stack images into a single tensor\n",
    "images_tensor = torch.stack(images)\n",
    "\n",
    "# Save the dataset\n",
    "dataset = {\n",
    "    \"images\": images_tensor,\n",
    "    \"metadata\": metadata_tensor,\n",
    "}\n",
    "torch.save(dataset, output_file)\n",
    "\n",
    "print(f\"Dataset saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10015/10015 [02:01<00:00, 82.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: {0: 1024, 1: 484, 2: 109, 3: 1074, 4: 5954, 5: 131, 6: 301}\n",
      "Balanced dataset saved to HAM10000Skin_Cancer_Balanced.pt\n",
      "Final dataset size: 21000 images and 21000 metadata entries\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "metadata_path = \"ham_metadata.csv\"\n",
    "images_path = \"HAMImages/\"\n",
    "output_file = \"HAM10000Skin_Cancer_Balanced.pt\"\n",
    "\n",
    "# Load the metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Dictionary for mapping target labels\n",
    "dx_mapping = {\n",
    "    \"bkl\": 0,\n",
    "    \"bcc\": 1,\n",
    "    \"df\": 2,\n",
    "    \"mel\": 3,\n",
    "    \"nv\": 4,\n",
    "    \"vasc\": 5,\n",
    "    \"akiec\": 6,\n",
    "}\n",
    "\n",
    "# Map 'dx' column to numerical labels\n",
    "metadata[\"dx\"] = metadata[\"dx\"].map(dx_mapping)\n",
    "\n",
    "# One-hot encode dx_type, sex, and localization columns\n",
    "metadata = pd.get_dummies(metadata, columns=[\"dx_type\", \"sex\", \"localization\"], prefix=[\"dx_type\", \"sex\", \"local\"])\n",
    "metadata = metadata.replace({True: 1, False: 0})\n",
    "\n",
    "# Handle missing age values using Iterative Imputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "imputer = IterativeImputer(estimator=LinearRegression(), max_iter=10, random_state=0)\n",
    "metadata['age'] = imputer.fit_transform(metadata[['age']])\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),         # Convert to tensor\n",
    "])\n",
    "\n",
    "# Initialize lists for storing processed data\n",
    "images = []\n",
    "metadata_list = []\n",
    "class_counts = {k: 0 for k in dx_mapping.values()}  # Track class counts\n",
    "nfc = 0\n",
    "fc = 0\n",
    "\n",
    "# Iterate through the metadata\n",
    "for _, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    image_path = os.path.join(images_path, row[\"image_id\"] + \".jpg\")\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "            metadata_list.append(row.drop([\"image_id\", \"lesion_id\"]).values)\n",
    "            class_counts[row[\"dx\"]] += 1\n",
    "            fc += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "    else:\n",
    "        nfc += 1\n",
    "\n",
    "# Convert to tensors\n",
    "metadata_tensor = torch.tensor(metadata_list, dtype=torch.float32)\n",
    "images_tensor = torch.stack(images)\n",
    "\n",
    "print(f\"Original class distribution: {class_counts}\")\n",
    "\n",
    "# Balance classes\n",
    "balanced_images = []\n",
    "balanced_metadata = []\n",
    "\n",
    "for cls, count in class_counts.items():\n",
    "    # Get indices of the class\n",
    "    cls_indices = [i for i, row in enumerate(metadata_tensor) if int(row[0]) == cls]\n",
    "    if count > 3000:\n",
    "        # Randomly select 1000 samples if the class has more\n",
    "        selected_indices = random.sample(cls_indices, 3000)\n",
    "    else:\n",
    "        # Oversample to 1000 if the class has fewer samples\n",
    "        selected_indices = resample(cls_indices, replace=True, n_samples=3000, random_state=0)\n",
    "    balanced_images.extend(images_tensor[selected_indices])\n",
    "    balanced_metadata.extend(metadata_tensor[selected_indices])\n",
    "\n",
    "# Convert balanced data back to tensors\n",
    "balanced_images_tensor = torch.stack(balanced_images)\n",
    "balanced_metadata_tensor = torch.stack(balanced_metadata)\n",
    "\n",
    "# Save the dataset\n",
    "dataset = {\n",
    "    \"images\": balanced_images_tensor,\n",
    "    \"metadata\": balanced_metadata_tensor,\n",
    "}\n",
    "torch.save(dataset, output_file)\n",
    "\n",
    "print(f\"Balanced dataset saved to {output_file}\")\n",
    "print(f\"Final dataset size: {len(balanced_images_tensor)} images and {len(balanced_metadata_tensor)} metadata entries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dasad\\AppData\\Local\\Temp\\ipykernel_14232\\1808705937.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dataset = torch.load(\"HAM10000Skin_Cancer_Balanced.pt\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16515072000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataset \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHAM10000Skin_Cancer_Balanced.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Extract images and metadata\u001b[39;00m\n\u001b[0;32m      7\u001b[0m images \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1360\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1361\u001b[0m             opened_zipfile,\n\u001b[0;32m   1362\u001b[0m             map_location,\n\u001b[0;32m   1363\u001b[0m             pickle_module,\n\u001b[0;32m   1364\u001b[0m             overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1365\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1366\u001b[0m         )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1368\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1848\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[0;32m   1847\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[1;32m-> 1848\u001b[0m result \u001b[38;5;241m=\u001b[39m unpickler\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1849\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1851\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1812\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1811\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1812\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m load_tensor(\n\u001b[0;32m   1813\u001b[0m         dtype, nbytes, key, _maybe_decode_ascii(location)\n\u001b[0;32m   1814\u001b[0m     )\n\u001b[0;32m   1816\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\serialization.py:1772\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1769\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset : storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[0;32m   1770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1771\u001b[0m     storage \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1772\u001b[0m         zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39mUntypedStorage)\n\u001b[0;32m   1773\u001b[0m         \u001b[38;5;241m.\u001b[39m_typed_storage()\n\u001b[0;32m   1774\u001b[0m         \u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[0;32m   1775\u001b[0m     )\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[0;32m   1777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 16515072000 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the dataset\n",
    "dataset = torch.load(\"HAM10000Skin_Cancer_Balanced.pt\")\n",
    "\n",
    "# Extract images and metadata\n",
    "images = dataset[\"images\"]\n",
    "metadata = dataset[\"metadata\"]\n",
    "\n",
    "# The target labels are the first column of metadata (assuming dx was mapped first)\n",
    "labels = metadata[:, 0].long()\n",
    "\n",
    "# Get the total number of samples\n",
    "total_samples = len(labels)\n",
    "\n",
    "# Count the number of samples in each class\n",
    "class_counts = torch.bincount(labels)\n",
    "\n",
    "# Get the size of the image tensor\n",
    "image_size = images.size()\n",
    "\n",
    "# Display results\n",
    "print(f\"Total samples: {total_samples}\")\n",
    "print(f\"Image size (C x H x W): {image_size[1:]}\")\n",
    "print(\"Samples per class:\")\n",
    "for class_id, count in enumerate(class_counts):\n",
    "    print(f\"  Class {class_id}: {count.item()} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10015/10015 [02:43<00:00, 61.09it/s]\n",
      " 93%|█████████▎| 43650/46935 [11:05<27:43,  1.98it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import random\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Paths\n",
    "metadata_path = \"ham_metadata.csv\"\n",
    "images_path = \"HAMImages/\"\n",
    "output_file = \"HAM10000Skin_Cancer_Balanced_with_SMOTE.pt\"\n",
    "\n",
    "# Load the metadata\n",
    "metadata = pd.read_csv(metadata_path)\n",
    "\n",
    "# Dictionary for mapping target labels\n",
    "dx_mapping = {\n",
    "    \"bkl\": 0,\n",
    "    \"bcc\": 1,\n",
    "    \"df\": 2,\n",
    "    \"mel\": 3,\n",
    "    \"nv\": 4,\n",
    "    \"vasc\": 5,\n",
    "    \"akiec\": 6,\n",
    "}\n",
    "\n",
    "# Map 'dx' column to numerical labels\n",
    "metadata[\"dx\"] = metadata[\"dx\"].map(dx_mapping)\n",
    "\n",
    "# One-hot encode dx_type, sex, and localization columns\n",
    "metadata = pd.get_dummies(metadata, columns=[\"dx_type\", \"sex\", \"localization\"], prefix=[\"dx_type\", \"sex\", \"local\"])\n",
    "metadata = metadata.replace({True: 1, False: 0})\n",
    "\n",
    "# Handle missing age values using Iterative Imputer\n",
    "imputer = IterativeImputer(estimator=LinearRegression(), max_iter=10, random_state=0)\n",
    "metadata['age'] = imputer.fit_transform(metadata[['age']])\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),         # Convert to tensor\n",
    "])\n",
    "\n",
    "# Separate out the image_id for later handling\n",
    "# Separate out the image_id for later handling\n",
    "image_ids = metadata['image_id']\n",
    "\n",
    "# Drop non-numeric columns (e.g., image_id, and any categorical columns)\n",
    "# Keep only numeric columns for SMOTE\n",
    "metadata_no_id = metadata.drop(columns=['image_id'])\n",
    "\n",
    "# Ensure only numeric columns are used in SMOTE (exclude non-numeric columns like 'dx', 'image_id', etc.)\n",
    "numeric_columns = metadata_no_id.select_dtypes(include=[np.number]).columns\n",
    "metadata_numeric = metadata_no_id[numeric_columns]\n",
    "\n",
    "# Apply SMOTE to clinical data (excluding 'image_id')\n",
    "smote = SMOTE(random_state=0)\n",
    "synthetic_data, _ = smote.fit_resample(metadata_numeric, metadata['dx'])\n",
    "\n",
    "# Add 'image_id' back to the synthetic data, keeping the original data intact\n",
    "synthetic_data = pd.DataFrame(synthetic_data, columns=numeric_columns)\n",
    "synthetic_data['image_id'] = np.nan  # Mark synthetic samples with NaN\n",
    "\n",
    "# Append the synthetic data at the end of the original data\n",
    "balanced_metadata = pd.concat([metadata, synthetic_data], ignore_index=True)\n",
    "\n",
    "\n",
    "# Track the class distribution\n",
    "class_counts = balanced_metadata['dx'].value_counts()\n",
    "\n",
    "# Initialize lists for processed images and their metadata\n",
    "images = []\n",
    "metadata_list = []\n",
    "\n",
    "# Process original (real) images\n",
    "for _, row in tqdm(metadata.iterrows(), total=len(metadata)):\n",
    "    image_path = os.path.join(images_path, row[\"image_id\"] + \".jpg\")\n",
    "    if os.path.exists(image_path):\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image = transform(image)\n",
    "            images.append(image)\n",
    "            metadata_list.append(row.drop([\"image_id\"]).values)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# Process synthetic images by augmenting real images\n",
    "for _, row in tqdm(synthetic_data.iterrows(), total=len(synthetic_data)):\n",
    "    if pd.isna(row['image_id']):  # Check if it's a synthetic row\n",
    "        # Find matching class in the original metadata\n",
    "        matching_rows = metadata[metadata['dx'] == row['dx']]\n",
    "        real_row = matching_rows.sample(1).iloc[0]  # Randomly select a real sample from the matching class\n",
    "        \n",
    "        # Load the corresponding image and apply random augmentation\n",
    "        real_image_path = os.path.join(images_path, real_row[\"image_id\"] + \".jpg\")\n",
    "        if os.path.exists(real_image_path):\n",
    "            try:\n",
    "                real_image = Image.open(real_image_path).convert(\"RGB\")\n",
    "                augmented_image = transform(real_image)\n",
    "                \n",
    "                # Apply random augmentation (random horizontal flip and color jitter for diversity)\n",
    "                if random.random() > 0.5:\n",
    "                    augmented_image = transforms.RandomHorizontalFlip()(augmented_image)\n",
    "                augmented_image = transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)(augmented_image)\n",
    "                \n",
    "                # Append the augmented image and the synthetic metadata\n",
    "                images.append(augmented_image)\n",
    "                metadata_list.append(row.drop([\"image_id\"]).values)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing augmented image: {e}\")\n",
    "\n",
    "# Convert to tensors\n",
    "metadata_tensor = torch.tensor(metadata_list, dtype=torch.float32)\n",
    "images_tensor = torch.stack(images)\n",
    "\n",
    "# Balance the dataset to 3000 samples per class\n",
    "final_images = []\n",
    "final_metadata = []\n",
    "\n",
    "# Ensuring every class has exactly 3000 samples\n",
    "for cls in class_counts.index:\n",
    "    cls_indices = [i for i, row in enumerate(metadata_tensor) if int(row[0]) == cls]\n",
    "    \n",
    "    # Oversample and undersample to make sure each class has 3000 samples\n",
    "    if len(cls_indices) > 3000:\n",
    "        selected_indices = random.sample(cls_indices, 3000)\n",
    "    else:\n",
    "        selected_indices = resample(cls_indices, replace=True, n_samples=3000, random_state=0)\n",
    "    \n",
    "    final_images.extend(images_tensor[selected_indices])\n",
    "    final_metadata.extend(metadata_tensor[selected_indices])\n",
    "\n",
    "# Convert final balanced dataset to tensors\n",
    "final_images_tensor = torch.stack(final_images)\n",
    "final_metadata_tensor = torch.stack(final_metadata)\n",
    "\n",
    "# Save the dataset\n",
    "dataset = {\n",
    "    \"images\": final_images_tensor,\n",
    "    \"metadata\": final_metadata_tensor,\n",
    "}\n",
    "torch.save(dataset, output_file)\n",
    "\n",
    "print(f\"Balanced dataset saved to {output_file}\")\n",
    "print(f\"Final dataset size: {len(final_images_tensor)} images and {len(final_metadata_tensor)} metadata entries\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
